{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwHFy6ArrBIjHMimSm14O1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrodoBaggins87/Machine_Learning/blob/main/Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###All important libraries/modules that may be needed:\n",
        "1. torchvision\n",
        "2. torchvision.datasets: has a range of toy datasets for different purposes\n",
        "3. torchvision.models:\n",
        "4. torchvision.transforms\n",
        "5. torch.utils.data.Dataset\n",
        "6. torch.utils.data.Dataloader"
      ],
      "metadata": {
        "id": "pb9-qRn7kqTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11JnWVv2jsnJ",
        "outputId": "dc39ca41-32c6-452b-9f5c-d60e97e522ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.0.post0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G63ze2LBbP0i",
        "outputId": "b0cd7772-de0b-4401-8579-332c543b98c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.1.0+cu121 Torchvision version: 0.16.0+cu121\n"
          ]
        }
      ],
      "source": [
        "#getting libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "#import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "#import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#import accuracy fuction\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "\n",
        "#chacking versions\n",
        "print(\"PyTorch version:\", torch.__version__, \"Torchvision version:\", torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Getting Training and Testing Data from Torchvision.datasets"
      ],
      "metadata": {
        "id": "QtgGGQEfoLps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=datasets.FashionMNIST(root=\"Fashion_Data\",#telling where to store the data\n",
        "                          train=True,#specifying if its training data or testing data\n",
        "                          download=True,#to download or not in case data not already present\n",
        "                          transform=ToTensor(),#which function to use to tranform PIL image into tensor\n",
        "                          target_transform=None#can transform labels using this#not necessary to use here as we are just assigning None value to it(labels dont need transformation)\n",
        "                          )\n",
        "test_data=datasets.FashionMNIST(root=\"Fashion_Data\",#telling where to store the data\n",
        "                          train=False,#specifying if its training data or testing data\n",
        "                          download=True,#to download or not in case data not already present\n",
        "                          transform=ToTensor(),#which function to use to tranform PIL image into tensor\n",
        "                          target_transform=None#can transform labels using this\n",
        "                          )"
      ],
      "metadata": {
        "id": "D8UZTbS6oKzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3e9ffb-4329-49ef-dc50-e21c828d9cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to Fashion_Data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 17487590.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Fashion_Data/FashionMNIST/raw/train-images-idx3-ubyte.gz to Fashion_Data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to Fashion_Data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 300626.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Fashion_Data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to Fashion_Data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to Fashion_Data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5497748.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Fashion_Data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to Fashion_Data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to Fashion_Data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13657354.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Fashion_Data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to Fashion_Data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data, label= train_data[12]\n",
        "data.shape, label #output tells shape of data and the label which is an integer\n",
        "#first index of shape gives number of colour channels, if its 3 then image is in RGB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1PuRGJxtOtK",
        "outputId": "5aeff58e-322d-43ae-9c70-a6a6a63547f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), 5)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Length of Train and Test dataset"
      ],
      "metadata": {
        "id": "EhVqQVw_upnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.__len__(),test_data.__len__() #find dataset specific methods from source code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7YcSsxGvl1s",
        "outputId": "6bacdcff-0829-4d59-9b95-2b4b39d02698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=train_data.classes #returns a list of names of classes"
      ],
      "metadata": {
        "id": "DCjzwjWG5R5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the data in dataset labels"
      ],
      "metadata": {
        "id": "7YKJlsPn0Pwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels=[]\n",
        "for i in range(train_data.__len__()):\n",
        "  data,label=train_data[i]\n",
        "  labels.append(label)\n",
        "set(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt1ri0WcxlTN",
        "outputId": "c5f984eb-e2c0-4551-e73c-cb0fe3e3a39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing Data"
      ],
      "metadata": {
        "id": "kkduyJdC0kGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image,label=train_data[5555]\n",
        "print(f\"Image Shape:{data.shape}\")\n",
        "image=image.view(image.shape[1],image.shape[2], image.shape[0])#putting colour map in  the last index so imshow doesnt show any error\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(class_names[label])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "VWajOa020mWn",
        "outputId": "65488881-cbd0-4539-fa27-a206f35aef21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Shape:torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Bag')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making Batches\n"
      ],
      "metadata": {
        "id": "PH8Gc3elNgMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#for large datasets its computationally faster to work in batches rather than working with individual datasets\n",
        "#setting batch size hyperparameter\n",
        "BATCH_SIZE= 32\n",
        "\n",
        "train_dataloader=DataLoader(train_data,#target dataset\n",
        "                            batch_size=BATCH_SIZE,#batch size\n",
        "                            shuffle=True)#whether to shuffle the datapoints or not so that neural network does not get trained on some unobvious pattern present in ordering of data points\n",
        "test_dataloader=DataLoader(test_data,\n",
        "                           batch_size=BATCH_SIZE,\n",
        "                           shuffle=False)#can make it shuffle but its not necessary and will add another calculation step in every epoch\n",
        "print(train_dataloader, test_dataloader)\n",
        "print(len(train_dataloader), len(test_dataloader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JiinuZPNxPy",
        "outputId": "aff405c2-5c1c-46aa-c987-2b2fa48d5acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7e1f0a0af730> <torch.utils.data.dataloader.DataLoader object at 0x7e1f0a0ade40>\n",
            "1875 313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking data inside data loader\n",
        "train_features_batch, train_labels_batch= next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACM7NkBiSql9",
        "outputId": "5db5b106-4ebc-460c-e151-74f11e5c888f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build a baseline model\n",
        "A baseline model is the simplest model which you start with before progressing to more complicated ones."
      ],
      "metadata": {
        "id": "hDDNttMY0Ad7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, model has one difference from previous models. A nn.Flatten layer is used to convert the data in 2 dimensions (of the image ) to one feature vector.\n",
        "The data in format[C,H,W] gets converted to [C,H*W].\n",
        "This is done because nn.Linear layer works better with vectors"
      ],
      "metadata": {
        "id": "j5-fThXi0vXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to see how flatten layer works, create a flatten layer\n",
        "flatten_layer=nn.Flatten()\n",
        "x=train_features_batch[0]\n",
        "output=flatten_layer(x)\n",
        "x.shape, output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIulpgni1odb",
        "outputId": "7758789b-56dd-4c66-f6cc-b52699f575ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), torch.Size([1, 784]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#building the baseline model\n",
        "from torch import nn\n",
        "class Fashion_Model_0(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units:int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential(nn.Flatten(),\n",
        "                              nn.Linear(in_features=input_shape, out_features= hidden_units),\n",
        "                              nn.Linear(in_features=hidden_units, out_features= output_shape))\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "wS-BBFu92xKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making first instance of the model"
      ],
      "metadata": {
        "id": "7o6JAoS78PD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0=Fashion_Model_0(input_shape=784,#as images here are 28x28=784, its hardcoded in this model\n",
        "                        hidden_units=50,\n",
        "                        output_shape=len(class_names))#output units will give one logit for wach member of class names\n",
        "model_0.to('cpu')#keep model in cpu to begin with"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUtwi-C88MQG",
        "outputId": "733c268b-1ce8-48ff-ff61-963f7bf2a17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Fashion_Model_0(\n",
              "  (layers): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=50, bias=True)\n",
              "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Accuracy, Loss function and Optimizer"
      ],
      "metadata": {
        "id": "PipRiFiG-s76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=MulticlassAccuracy(num_classes=len(class_names))\n",
        "loss_fn=nn.CrossEntropyLoss()#same as multiclass classification loss function\n",
        "optimizer=torch.optim.SGD(params=model_0.parameters(), lr=0.1)#SGD optimizer"
      ],
      "metadata": {
        "id": "AKvrh6Nu-zTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining function to measure time taken for training model"
      ],
      "metadata": {
        "id": "1vTycCl8lH_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start:float, end: float, device: torch.device=None):\n",
        "\n",
        "  \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "  total_time = end - start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "31RUq-LIlMie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making training and testing loop"
      ],
      "metadata": {
        "id": "I_Kiq3xDqRMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm# for showing progress meter"
      ],
      "metadata": {
        "id": "HNvz7SXdqUSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "epochs=3#keep small at first for smaller loop time\n",
        "#set random seed\n",
        "torch.manual_seed(56)\n",
        "#start the timer\n",
        "time_start_cpu=timer()\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(\"Epoch:\", epoch)\n",
        "  ##Training\n",
        "  train_loss=0\n",
        "  model_0.train()\n",
        "  #loop through training batches\n",
        "  for batch,(x,y) in enumerate(train_dataloader):\n",
        "    #1. Forward Pass\n",
        "    y_pred=model_0(x)\n",
        "\n",
        "    #2. Calculate Loss\n",
        "    loss= loss_fn(y_pred, y)\n",
        "    train_loss+=loss#add up the loss of each sample in the batch to get loss of whole batch\n",
        "\n",
        "    #3. optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #4. Loss Backward\n",
        "    loss.backward()\n",
        "\n",
        "    #5. Optimizer Step\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch%50==0:\n",
        "      amount=(batch * len(x))/len(train_dataloader.dataset)\n",
        "      print(\"Looked at\", amount, \"samples\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UtdbaNGZqZeC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}